# AI Models Load Balancer Configuration
upstream llama32_backend {
    server llama32:11434;
}

upstream gemma3_backend {
    server gemma3:11434;
}

upstream phi4_backend {
    server phi4:11434;
}

upstream deepseek_backend {
    server deepseek:11434;
}

upstream gpt_oss_backend {
    server gpt_oss:11434;
}

upstream qwen25_backend {
    server qwen25:11434;
}

# Rate limiting
limit_req_zone $binary_remote_addr zone=ai_requests:10m rate=10r/s;

server {
    listen 80;
    server_name ai-models.local;

    # Security headers
    add_header X-Frame-Options "SAMEORIGIN" always;
    add_header X-Content-Type-Options "nosniff" always;
    add_header X-XSS-Protection "1; mode=block" always;

    # Enable compression
    gzip on;
    gzip_types text/plain application/json;

    # Health check endpoint
    location /health {
        access_log off;
        return 200 "AI Models Gateway OK\n";
        add_header Content-Type text/plain;
    }

    # Llama 3.2 - General purpose, lightweight
    location /api/llama32/ {
        limit_req zone=ai_requests burst=20 nodelay;
        
        rewrite ^/api/llama32/(.*) /$1 break;
        proxy_pass http://llama32_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        # Timeout settings for AI inference
        proxy_connect_timeout 30s;
        proxy_send_timeout 300s;
        proxy_read_timeout 300s;
        
        # Buffer settings
        proxy_buffering on;
        proxy_buffer_size 8k;
        proxy_buffers 32 8k;
    }

    # Gemma 3 - Specialized tasks, efficient
    location /api/gemma3/ {
        limit_req zone=ai_requests burst=15 nodelay;
        
        rewrite ^/api/gemma3/(.*) /$1 break;
        proxy_pass http://gemma3_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        proxy_connect_timeout 30s;
        proxy_send_timeout 300s;
        proxy_read_timeout 300s;
    }

    # Phi-4 - Advanced reasoning and code
    location /api/phi4/ {
        limit_req zone=ai_requests burst=10 nodelay;
        
        rewrite ^/api/phi4/(.*) /$1 break;
        proxy_pass http://phi4_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        proxy_connect_timeout 60s;
        proxy_send_timeout 600s;
        proxy_read_timeout 600s;
    }

    # DeepSeek - Reasoning and analysis
    location /api/deepseek/ {
        limit_req zone=ai_requests burst=12 nodelay;
        
        rewrite ^/api/deepseek/(.*) /$1 break;
        proxy_pass http://deepseek_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
    proxy_set_header X-Forwarded-Proto $scheme;
        
        proxy_connect_timeout 45s;
        proxy_send_timeout 450s;
        proxy_read_timeout 450s;
    }

    # GPT-OSS - Open source alternative
    location /api/gpt-oss/ {
        limit_req zone=ai_requests burst=15 nodelay;
        
        rewrite ^/api/gpt-oss/(.*) /$1 break;
        proxy_pass http://gpt_oss_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        
        proxy_connect_timeout 45s;
        proxy_send_timeout 450s;
        proxy_read_timeout 450s;
    }

    # Qwen2.5 - PRO model (unified)
    location /api/pro/ {
        limit_req zone=ai_requests burst=20 nodelay;
        rewrite ^/api/pro/(.*) /$1 break;
        proxy_pass http://qwen25_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        proxy_connect_timeout 45s;
        proxy_send_timeout 450s;
        proxy_read_timeout 450s;
    }

    # Model selection API - routes to appropriate model based on task
    location /api/smart-route/ {
        limit_req zone=ai_requests burst=25 nodelay;
        
        # Custom logic would go here to route based on request type
        # For now, default to llama32 for general requests
        rewrite ^/api/smart-route/(.*) /$1 break;
        proxy_pass http://llama32_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
    }

    # Model status endpoint
    location /api/status {
        access_log off;
        return 200 '{
            "models": {
                "llama32": "active",
                "gemma3": "active", 
                "phi4": "active",
                "deepseek": "active",
                "gpt_oss": "active"
                , "pro": "active"
            },
            "gateway": "online",
            "timestamp": "$time_iso8601"
        }';
        add_header Content-Type application/json;
    }

    # Default location - show available endpoints
    location / {
        return 200 '{
            "message": "AI Models Gateway",
            "endpoints": {
                "llama32": "/api/llama32/",
                "gemma3": "/api/gemma3/",
                "phi4": "/api/phi4/",
                "deepseek": "/api/deepseek/",
                "gpt_oss": "/api/gpt-oss/",
                "pro": "/api/pro/",
                "smart_route": "/api/smart-route/",
                "status": "/api/status",
                "health": "/health"
            },
            "documentation": "https://github.com/ollama/ollama/blob/main/docs/api.md"
        }';
        add_header Content-Type application/json;
    }
}
